---
title: "Topic modelling"
author: "Garro"
date: "2024-03-19"
output: html_document
---

```{r}
library(tidyverse) |> suppressMessages()
library(tidytext)
library(tm) |> suppressMessages()
library(SnowballC) |> suppressMessages()
library(tidytext) |> suppressMessages()
library(discoverableresearch) |> suppressMessages()
library(readxl)
#library(discoverableresearch)
library(scraEP)
library(tm)
library(stringr)

stops_es <- c("largo","cada","sino","tambien","ser","ano","anos","dar", "measurable", "timebound","achievable","relevant","mediante","mas","cada","vez","n",
              "puedan","busca","realizar","tal","deben","tres","cuatro","uno","dos","cinco","seis","siete","ocho","nueve","mil","solo","hace","asi","ad","re",
              "do","das","ahora bien",
              unaccent(stopwords(kind="es")))

#function to preprocess text, but will be u
preprocess_text <- function(text) {
    text <- tolower(text)  # Convert to lowercase
    text <- gsub("[[:punct:]]", "", text)  # Remove punctuation
    text <- gsub("[[:digit:]]", "", text)  # Remove numbers
    text <- gsub("[ \t\r\n]+", " ", text)  # Remove extra whitespaces
    text <- trimws(text)  # Remove leading and trailing whitespaces
    text <- unaccent(text)
    text <- removeWords(text, stops_es)  # Remove stopwords
    text <- wordStem(text, language = "spanish")  # Stemming
}
```


Topic modelling


```{r}
#read the data
data <- readRDS("C:/Users/carme/Dropbox/MIO/DOCS/Hertie/Thesis/Elecciones_Municipales_2024/plans.rds")
#new generacion cartago
data$Text[293] = ""
#canas - unidos podemos
data$Text[329] = ""
# curri siglo xxi
data$Text[479] = ""
#mora lib nac
#data[517,]
data$Text[517] = ""
#moravia partido somos moravia
data$Text[525] = ""
#paste(unlist(data$Text[250]), collapse = " ")
```


Take only the relevant part of the texts: the proposals.
```{r}
posi <- read_xlsx("C:/Users/carme/Dropbox/MIO/DOCS/Hertie/Thesis/Elecciones_Municipales_2024/donde_comienzan.xlsx")
data$Propuestas <- ""

for(i in seq(1,566)){
  if(!is.na(posi$begins[i])){
    if(!is.na(posi$ends[i])){
      data$Propuestas[i] = list(unlist(data$Text[i])[posi$begins[i]:posi$ends[i]])
    } else{
      data$Propuestas[i] = list(unlist(data$Text[i])[posi$begins[i]:length(unlist(data$Text[i]))])
    }
  }
}
  
#posi[220:230,]  
```


```{r}

data <- data |> 
  mutate(clean_text = ifelse(is.na(Propuestas), "", Propuestas))

# go from a vector in each cell to a full text for each of the documents
for(i in seq(1,nrow(data))){
  data$clean_text[i] = paste(unlist(data$Propuestas[i]), collapse = " ")
  # mejor sacar primero los n grams
  #data$clean_text[i] = preprocess_text(data$clean_text[i])
}


#data$clean_text <- preprocess_text(data$clean_text)

#data[220:230,]
```

Add column that states if they uploaded the plan, if it was readable or if it was not uploaded.

```{r}
View(data)

data <- data |> mutate(action = case_when(is.na(Location) ~ "No plan",
                                          !is.na(Location) & clean_text == '' ~ "Non readable",
                                          clean_text != '' ~ 'Readable text'))
```



#Hasta ac√°

Me voy a pasar a python porque los bigrams are being a pain :P

```{r}
#View(data)
for_csv <- data |> select(Provincia, Municipalidad, Partido, Candidato, Ganador, Votos, Participacion, Abstencionismo, clean_text, action)
for_csv <- for_csv |> mutate(clean_text = unlist(clean_text))

#View(for_csv)
write.csv(for_csv,"C:/Users/carme/Dropbox/MIO/DOCS/Hertie/Thesis/Elecciones_Municipales_2024/clean_data.csv")
```



#######################################################################################################################


```{r}
View(for_csv)
```



Now, I will get the text with everything, with the purpose of getting relevant n_grams.
I think it makes sense to look for relevant bi-tri-quad-qint? grams.
We can filter by those that have n-1 stop words to get the more relevant ones.
Because there are so many documents, I think it makes sense to look for bi grams that appear more than 15 times.

```{r}
full_text <- paste(unlist(data$clean_text), collapse = " ")
```

Doing n-grams
```{r}
propuestas <- data |> select(Provincia, Municipalidad, Partido, Candidato, Ganador, Votos, Participacion, Abstencionismo, clean_text)

bigrams <- propuestas |> unnest_tokens(bigram, clean_text, token="ngrams", n=2) |> filter(!is.na(bigram))



bigrams <- bigrams |> count(bigram, sort = TRUE) |> 
  separate(bigram, c("word1", "word2"), sep = " ") |>
  filter(nchar(word1)>1,
         nchar(word2)>1) |>
  filter(!word1 %in% stops_es,
         !word2 %in% stops_es) |>
  unite(bigram,word1, word2, sep =" ")

bigrams <- bigrams |>
  mutate(new_word = gsub(" ","_", bigram))

head(bigrams)

#(data$clean_text[223])

text <- propuestas$clean_text[223]

for(pair in bigrams$bigram){
  underscore = gsub(" ","_",pair)
  text = gsub(pair,underscore,text)
}

text
```





```{r}
bigrams


```

