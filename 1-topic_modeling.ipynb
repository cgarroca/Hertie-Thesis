{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\carme\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\carme\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# for first part\n",
    "\n",
    "import pandas as pd\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import LdaModel\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import numpy as np\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import LdaModel, CoherenceModel\n",
    "import re\n",
    "import unidecode\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "\n",
    "#download puntuation and stopwords\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "#additional for second part\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install Unidecode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(310, 10)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('C:/Users/carme/Dropbox/MIO/DOCS/Hertie/Thesis/Hertie-Thesis/Data/clean_data.csv')\n",
    "# 'C:/Users/carme/Dropbox/MIO/DOCS/Hertie/Thesis/Elecciones_Municipales_2024/clean_data.csv'\n",
    "#filter column with na values\n",
    "plans = data.copy().dropna()\n",
    "plans = plans.reset_index(drop=True)\n",
    "plans.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete the one duplicate value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(309, 10)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encontrar duplicados en 'clean_text'\n",
    "duplicated_texts = plans['clean_text'][plans['clean_text'].duplicated(keep=False)]\n",
    "\n",
    "# Mostrar los documentos duplicados\n",
    "# print(duplicated_texts)\n",
    "\n",
    "# plans.loc[303,'clean_text']\n",
    "plans.at[303,'clean_text'] = float(\"NaN\")\n",
    "# plans.loc[303,'clean_text']\n",
    "plans = plans.dropna()\n",
    "plans.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove accents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    El ambiente nos importa\\r\\nLa preservacion del...\n",
      "1        A Quienes Habitan Alajuela\\r\\n    Alajuela...\n",
      "2                            PROGRAMA DE GOBIERNO\\r...\n",
      "3         PLAN DE GOBIERNO 2024 - 2028 | KATHIA ARR...\n",
      "4    DESARROLLO\\r\\nECONOMICO..........................\n",
      "Name: clean_text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#remove accents from spanish words\n",
    "def remove_accents(text):\n",
    "    return unidecode.unidecode(text)\n",
    "\n",
    "plans['clean_text'] = plans['clean_text'].apply(remove_accents)\n",
    "\n",
    "print(plans['clean_text'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete loose letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir la función para limpiar el texto\n",
    "def limpiar_texto(texto):\n",
    "    # Eliminar palabras de una sola letra que no sean 'a' o 'I' (ajustar según tu criterio)\n",
    "    texto_limpio = re.sub(r'\\b(?<!\\b[aAiI]\\b)[a-zA-Z]\\b', '', texto)\n",
    "    return texto_limpio\n",
    "\n",
    "# Aplicar la función a cada elemento de la columna de texto\n",
    "plans['clean_text'] = plans['clean_text'].apply(limpiar_texto)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To lower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plans['clean_text'] = plans['clean_text'].lower()\n",
    "plans['clean_text']=[text.lower() for text in plans['clean_text']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete province, canton and name of candidate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para eliminar palabras de las columnas especificadas de 'clean_text'\n",
    "def remove_words(row, column_names):\n",
    "    text = row['clean_text']\n",
    "    for column in column_names:\n",
    "        # Obtener palabras del campo actual, dividiendo por espacio o guión bajo\n",
    "        # La expresión regular [ _] significa \"espacio o guión bajo\"\n",
    "        # Usar str.lower() para convertir el texto a minúsculas antes de dividirlo\n",
    "        words = re.split(r'[ _]', str(row[column]).lower())\n",
    "        # Eliminar cada palabra, en sus diferentes formas de capitalización, del texto\n",
    "        for word in words:\n",
    "            text = re.sub(r'\\b' + re.escape(word) + r'\\b', '', text, flags=re.IGNORECASE)\n",
    "    return text\n",
    "\n",
    "    # Columnas de las cuales se eliminarán las palabras en 'clean_text'\n",
    "columns_to_remove = ['Provincia', 'Municipalidad', 'Partido', 'Candidato']\n",
    "\n",
    "# plans['clean_text'] = plans.apply(remove_words, column_names=columns_to_remove, axis=1)\n",
    "plans.loc[:, 'clean_text'] = plans.apply(lambda row: remove_words(row, columns_to_remove), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('spanish'))\n",
    "# Take accents from stopwords\n",
    "stop_words = {unidecode.unidecode(word) for word in stop_words}\n",
    "\n",
    "# Lista de palabras personalizadas para agregar\n",
    "custom_stop_words = [\n",
    "    'ana imelsa guzman espinoza','junnier a salazar tobal','claudia quintanilla tartaglione',\n",
    "    'accion',\n",
    "    'alajuelense', 'alcalde', 'alcalde', 'alcaldesa', 'alcaldia', 'alcaldia', 'ambito', 'anexo', 'anterior', 'apoyaremos', 'aqui',\n",
    "    'brenes', 'busca', 'busca', \n",
    "    'cabo', 'cabo', 'canaza', 'candidata','canton','cantonal', 'carara', 'caso', 'cion', 'cl', 'continuar', 'continuar', 'coordinaremos', 'cosas', 'crearemos', \n",
    "    'cuales', 'cuanto', 'cuanto', 'cuatro', 'comunidad','comunidades',\n",
    "    'debemos', 'debemos', \n",
    "    'elena', 'enrique', 'etc', 'etc', \n",
    "    'fomentaremos', 'formato', \n",
    "    'gestionaremos', 'golfito', 'guayabo', \n",
    "    'hagamoslo', 'haremos', \n",
    "    'ii', 'implementaremos', 'incluyendo', 'indicador',\n",
    "    'juntos', \n",
    "    'kathia arroyo', \n",
    "    'linea', 'lineas', 'llevar', \n",
    "    'maria', 'meta', \n",
    "    'ndeg', 'numero',\n",
    "    'objetivo', \n",
    "    'pagina', 'partir', 'periodo', 'permitira', 'pilar', 'plan de gobierno', 'poas', 'poasena', 'porcentaje', 'posible', 'posteriormente', \n",
    "    'primer', 'primera', 'promoveremos', 'proponemos', 'provincia', 'programa de gobierno','porque liberia debe avanzar',\n",
    "    'responde', 'rodriguez', \n",
    "    'sa', 'samara', 'san', 'seccion', 'sino',\n",
    "    'tabarcia', 'tal', 'tales', 'trabajaremos', 'tres', 'tribunal interno de elecciones',\n",
    "    'vamos', 'vamos', 'ver',\n",
    "    \"ejecutarlo\", \"nombre\", \"finalidad\", \"complementado\", \"instrumento\", \"debera\", \"fondos\", \"asegurar\",\n",
    "    \"verdes\", \"clave\", \"practicas\", \"potenciar\", \"asegurar\", \"habilidades\", \"ejes\", \"especial\", \"pueden\", \"nuevo\", \"asegurar\",\n",
    "    \"metas\", \"tipo\", \"permanente\", \"falta\" ,\"requiere\", \"atenienses\", \"rapidos\", \"ateniense\", \"empiecen\", \"impulso\", \"conlleva\", \"validar\",\n",
    "    \"colectivamente\", \"presta\", \"comprende\", \"orden\", \"anchas\", \"intereses\" , \"poner\", \"mencionamos\" , \"potenciar\", \"vote papeletas\", \"limonense\",\n",
    "    \"creara\", \"rio_blanco\", \"limonenses\", \"resultados\", \"resumen\", \"iniciativa\", \"asegurando\", \"clave\", \"asegurar\", \"fecha\", \"descritos\",\n",
    "    \"indicadores\", \"estrategico\", \"avanza\", \"continuaremos\", \"asimismo\", \"santa\", \"dia\", \"gracias\", \"realizadas\", \"aprovechamiento\", \"dad\", \"nes\",\n",
    "    \"impulsaremos\", \"buscaremos\", \"des\", \"municipes\", \"tos\", \"desarrollaremos\", \"pavones\", \"elecciones\", \"esparcimiento\", \"realizacion\", \"total\",\n",
    "    \"nuevo\", \"falta\", \"debido\", \"impulsaremos\", \"rutinario\", \"estrategicas\", \"menciona\", \"pag\", \"escazucenos\", \"rafael\", \"tes\", \"aportado\",\n",
    "    \"sirviendo\", \"pusc\", \"propone\", \"merecemos\", \"construyendo\", \"tienden\", \"siempre\", \"ameriten\", \"requeriran\", \"treinta\", \"beneficiado\", \"cuento\",\n",
    "    \"bicentenario\"]\n",
    "\n",
    "\n",
    "# Agregar las palabras personalizadas al conjunto de stopwords\n",
    "stop_words.update(custom_stop_words)\n",
    "\n",
    "#preprocess and stemming\n",
    "# def preprocess(text):\n",
    "#     return [word for word in word_tokenize(text.lower()) if word.\n",
    "#             isalpha() and word not in stop_words]\n",
    "\n",
    "# Spanish stemmer from Snowball\n",
    "stemmer = SnowballStemmer(\"spanish\")\n",
    "\n",
    "#with stemming and preprocessing\n",
    "def preprocess(text):\n",
    "    # Normalize and remove non-alpha characters\n",
    "    text = unidecode.unidecode(text.lower())\n",
    "    text = re.sub(r'[^a-z ]+', '', text)\n",
    "    # Tokenize, remove stopwords, and stem\n",
    "    tokens = word_tokenize(text)\n",
    "    # return ' '.join(stemmer.stem(word) for word in tokens if word not in stop_words) #with stemmer\n",
    "    return ' '.join(word for word in tokens if word not in stop_words) #with no stemmer\n",
    "\n",
    "plans['processed_text'] = plans['clean_text'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Include bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Construir modelos de bigramas y trigramas\n",
    "# bigram = Phrases(processed_docs, min_count=10, threshold=10)  # Ajusta los parámetros según sea necesario\n",
    "# bigram_mod = Phraser(bigram)\n",
    "\n",
    "# # Aplicar el modelo para construir bigramas\n",
    "# bigram_texts = [bigram_mod[doc] for doc in processed_docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic modeling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Crear el diccionario y el corpus para LDA\n",
    "# dictionary = Dictionary(bigram_texts)\n",
    "# dictionary.filter_extremes(no_below=2, no_above=0.5)\n",
    "# corpus = [dictionary.doc2bow(doc) for doc in bigram_texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dictionary and corpus for LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary = Dictionary(processed_docs)\n",
    "# dictionary.filter_extremes(no_below=2, no_above=0.5)\n",
    "# corpus = [dictionary.doc2bow(doc) for doc in processed_docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDA: Training and analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_topics = 150  # Ajusta este número según tus necesidades\n",
    "# lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics, passes=15)\n",
    "\n",
    "# for i, topic in lda_model.show_topics(formatted=True, num_topics=num_topics, num_words=10):\n",
    "#     print(f'Tema {i}: {topic}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not great. Possible reasons:\n",
    "+ lengths of documents are very different\n",
    "+ the docs are not very different between each other\n",
    "\n",
    "# Using TF-IDF. Otherwise we can try doing a Guided LDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic  0\n",
      "[('pensando grande', 0.3898240245831482), ('local areas', 0.2746915758651164), ('tacares', 0.21332041482546377), ('per capita', 0.20612819141733754), ('tasa mortalidad', 0.1993133864272263), ('capita', 0.19591388576373264), ('sarchi', 0.1891577129161759), ('municipaleje', 0.18012359259700947), ('ademas hacer', 0.17133341427346585), ('trabajo comites', 0.1711722166512821)]\n",
      "Topic  1\n",
      "[('social sector', 0.3050498240820449), ('proseguir', 0.24253651599245704), ('relanzar', 0.24072843663065366), ('cana', 0.2218025050439511), ('bienestar salud', 0.22017070618617077), ('eje economico', 0.2064935574100022), ('muni digital', 0.19857939930665464), ('juegos comunales', 0.19527243440101266), ('cantonalse', 0.192020692004463), ('sistema computo', 0.18830660998235196)]\n",
      "Topic  2\n",
      "[('evaluar', 1.6546902462587814), ('plp', 1.30532230318389), ('ultimos', 1.2248284407700392), ('promueve', 1.1070170275110451), ('satisfaccion', 1.029185280059857), ('jose', 0.9960766349660064), ('continuo', 0.948940096084766), ('transformar', 0.9417496639136116), ('regular', 0.9124411860266799), ('expertos', 0.9094992180866852)]\n",
      "Topic  3\n",
      "[('millones', 1.0925152432445189), ('francas', 0.9789045934641124), ('zonas francas', 0.9525444773494753), ('gestion vial', 0.8884080575626595), ('socio', 0.8285896494952897), ('emocional', 0.8265468405106889), ('juridica', 0.8115159407725474), ('orientadas', 0.7961846138206162), ('dinamica', 0.7818296282979216), ('mundial', 0.7635665596692434)]\n",
      "Topic  4\n",
      "[('ods', 1.5148361055765394), ('productivo', 1.262096510514617), ('acceso servicios', 1.2582515176484572), ('responsables', 1.0890632148877653), ('iii', 0.9346094573165178), ('inclusivos', 0.9088798257272802), ('politica publica', 0.9070322457595781), ('favor', 0.9038367564612105), ('impulsar desarrollo', 0.8508805975919906), ('posicion', 0.8485946839057982)]\n",
      "Topic  5\n",
      "[('programade', 0.5443192178766812), ('ra', 0.4461663241153558), ('ro', 0.3847058756306498), ('re', 0.26828807830171314), ('fo', 0.21631575217912571), ('huetar', 0.2141075364792896), ('laeficiencia eficacia', 0.2128490360579572), ('estrategias implementar', 0.21018540853443823), ('aduana', 0.2081071459169087), ('laeficiencia', 0.20256592064714649)]\n",
      "Topic  6\n",
      "[('quesada', 0.3561993337578733), ('acoso sexual', 0.28153954548494964), ('once', 0.2329050387736196), ('vice', 0.21373901694734096), ('once distritos', 0.20923870270994882), ('alta demanda', 0.19964914885297844), ('depto', 0.19863350086064416), ('funcionando', 0.19667219271750067), ('caminos publicos', 0.19170014074331723), ('zmt', 0.19123291891710803)]\n",
      "Topic  7\n",
      "[('responsable municipal', 0.39784218701558327), ('municipal afectado', 0.37639647395547493), ('plan desarrollar', 0.34228923207617223), ('pnp', 0.2650509223968447), ('ilustracion', 0.2518066066343345), ('bataan', 0.23399650626733104), ('sosteniblesobjetivo', 0.21686185213852704), ('carrandi', 0.18958900274222404), ('industriales zonas', 0.18441239397211923), ('presente eje', 0.1811900086445855)]\n",
      "Topic  8\n",
      "[('pnp', 0.42141244746972706), ('anos anos', 0.22954742264935954), ('realizar menos', 0.18553182737760082), ('componentes claves', 0.1595870194754274), ('situacion riesgo', 0.13055978568801038), ('evaluen', 0.11915490131127462), ('belemitas', 0.11460065343639544), ('construccion programas', 0.1114857326276025), ('categoria delictiva', 0.10907533494069022), ('delictiva cantidad', 0.10907533494069022)]\n",
      "Topic  9\n",
      "[('departamento', 1.418395519261567), ('biodiversidad', 1.360070853136062), ('carreteras', 1.3199055027834068), ('equipamiento', 1.2960998913300101), ('junta', 1.2784962812865561), ('transformacion', 1.273005699709873), ('desastres', 1.1943420058856749), ('inmediata', 1.170921558944128), ('medica', 1.1357618872788997), ('promocionar', 1.1295625557394837)]\n",
      "Topic  10\n",
      "[('municipalidad plan', 0.37871769073165623), ('salon comunal', 0.3768760504181983), ('gobierno version', 0.25873560383257965), ('meses gestion', 0.23959370368907912), ('grupos trabajo', 0.23634305354892027), ('energetico', 0.21870929006620368), ('parritena', 0.2041358344814864), ('depto', 0.20255968981836595), ('fraccion', 0.19755092581695552), ('escuelas colegio', 0.19452203199821902)]\n",
      "Topic  11\n",
      "[('propuesta establecer', 0.24833634488944298), ('propuesta implementar', 0.2152005688903193), ('tendiente', 0.19497798671619385), ('contenidas', 0.16159517446101618), ('plan acciones', 0.15995684637384922), ('general implementar', 0.15035288007663355), ('vincularlas', 0.14207401088611024), ('municipal estrategias', 0.14207400851377083), ('minuciosamente', 0.14207400490556893), ('analizadas', 0.1420739999023482)]\n",
      "Topic  12\n",
      "[('mil', 0.651038444925065), ('degobierno', 0.6066882104270064), ('ejecutados', 0.5928575801172256), ('fi', 0.5176352090141356), ('corrientes', 0.4917886628836007), ('programa degobierno', 0.4616428358127763), ('municipalidad altura', 0.45421866283890233), ('formacion profesional', 0.42860349141553394), ('elementos estrategicos', 0.4068874714756904), ('lider', 0.3893452505385933)]\n",
      "Topic  13\n",
      "[('caminos', 2.726535220946202), ('animal', 2.6788824211398308), ('plazo', 2.6637800862731074), ('datos', 2.3483665346471727), ('impuestos', 2.066425207449126), ('alianza', 1.854759465362921), ('anual', 1.7865129693240318), ('principal', 1.7433722210161267), ('propiciar', 1.741485792525628), ('diversidad', 1.649945628439145)]\n",
      "Topic  14\n",
      "[('gestion', 14.345515801786899), ('gobierno', 14.14637229976298), ('municipalidad', 13.377551681537057), ('personas', 13.24220847914539), ('servicios', 11.599240529441667), ('promover', 11.329837709979826), ('programas', 10.716583617247489), ('proyectos', 10.619471212237405), ('local', 10.272463433673279), ('recursos', 9.877309641299425)]\n",
      "Topic  15\n",
      "[('gobierno municipal', 3.9786574590297663), ('tener', 3.196612540415932), ('poder', 3.1342887887985595), ('evaluacion', 2.969148698020949), ('propuestas', 2.8644529065538054), ('tiempo', 2.862105822833058), ('menos', 2.6916420363606197), ('siguientes', 2.689282805008018), ('animales', 2.6371451190810133), ('instituto', 2.624108425202384)]\n",
      "Topic  16\n",
      "[('establecer sistema', 0.4109490130396475), ('siguientes estrategias', 0.2735490708955095), ('gobernanza municipal', 0.24974037382524245), ('sistema seguimiento', 0.24407819786961835), ('plan gobernanza', 0.23879173957144334), ('realizar seguimiento', 0.22346958673082784), ('general mejorar', 0.20988067974525515), ('lograr proyectos', 0.19408054290219234), ('jerarcas', 0.1911426066119742), ('va hacer', 0.1827580366627579)]\n",
      "Topic  17\n",
      "[('propuestaseje', 0.3012874975235815), ('camino progreso', 0.24425706661867114), ('estudiantes puedan', 0.19718963044240803), ('area construccion', 0.18666751509726537), ('podria ser', 0.1808930894288581), ('servicios forma', 0.17485467092342608), ('personas alajuelenses', 0.17282595279881877), ('ess', 0.1703188531866518), ('buscamos mejorar', 0.1693114260653339), ('norte sur', 0.1676055508874302)]\n",
      "Topic  18\n",
      "[('grupo trabajo', 0.30520020149870686), ('lic', 0.25980545973331653), ('politica plan', 0.22574320374831774), ('casapresidencial', 0.21105132364630505), ('ict canatur', 0.20278381951639088), ('volver ser', 0.20132516510098727), ('puente entrada', 0.1992853742072568), ('oportunidadesinversion', 0.19084024570336702), ('elaprovechamiento recursos', 0.19084023251687804), ('ambientalproteccion', 0.19084023247792517)]\n",
      "Topic  19\n",
      "[('elementos especificos', 0.8212389503262356), ('mipymes', 0.626821212626862), ('reforma', 0.6159954008784793), ('salon', 0.5696985468491007), ('otorgar', 0.5653082141811169), ('semestre', 0.5241449211617659), ('desarrollos', 0.4869197034011541), ('horario', 0.48272033136722836), ('comportamiento', 0.4647987021695991), ('gestion institucional', 0.46064545567305026)]\n",
      "Topic  20\n",
      "[('programacion', 1.2775160685575886), ('humana', 1.0065387441428875), ('vista', 0.9012084033575021), ('algun', 0.8669849459309364), ('envejecimiento', 0.8242127252770595), ('recursos humanos', 0.7255610616830077), ('progresa', 0.6823322551235222), ('hacerlo', 0.6364527005028093), ('accidentes', 0.5873852700120983), ('enlas', 0.5781323864397766)]\n",
      "Topic  21\n",
      "[('podemos hacerlo', 0.3391714847977624), ('pregunta', 0.32966870432471446), ('gobierno programa', 0.3296150236605909), ('gobierno debe', 0.2907606365525677), ('mil personas', 0.26664797185923705), ('cumplimiento revision', 0.24800410067889395), ('convenios cooperativos', 0.19417947328760843), ('articulacion estrategica', 0.1911829841460222), ('debe avanzar', 0.165895243572933), ('municipalplan', 0.14066838184411204)]\n",
      "Topic  22\n",
      "[('gobierno version', 0.4030499487582533), ('destacadas', 0.23652221436548404), ('caminos calles', 0.2282991359536747), ('ss', 0.2186635016668764), ('cahuita', 0.198231582830213), ('destaque', 0.19493389222150267), ('capacidad disfrutar', 0.1584514529310884), ('amplia capacidad', 0.1584514529310884), ('comision compuesta', 0.14962139794292967), ('distrito cahuita', 0.14906966964070273)]\n",
      "Topic  23\n",
      "[('ifam', 0.9058750479944635), ('organizaciones sociales', 0.8148285902047547), ('eje desarrollo', 0.7802417280696097), ('personas mayores', 0.7109081583381199), ('historica', 0.677848386219141), ('colon', 0.6665917184065806), ('vida poblacion', 0.6334623444244729), ('recomendaciones', 0.6200333463085373), ('apego', 0.6187900235701269), ('to', 0.559552262947139)]\n",
      "Topic  24\n",
      "[('jaco', 0.16777306698232541), ('herradura', 0.15901608464246075), ('lagunillas', 0.14782850707053413), ('transformadora', 0.12705682053673845), ('mariscos', 0.12649451951281626), ('mediante busqueda', 0.12297538215067394), ('descansar', 0.1199484185605279), ('resi', 0.11808992384994324), ('miercoles', 0.11780881900702125), ('uruguay', 0.11570070051866777)]\n",
      "Topic  25\n",
      "[('medira', 0.3001784161874358), ('exito proyecto', 0.24583091018781938), ('detallan continuacion', 0.19604440883684823), ('anos gobierno', 0.19166962637953475), ('sigma', 0.17690386604552083), ('six', 0.17690386604552083), ('six sigma', 0.17690386604552083), ('lean six', 0.17690386604552083), ('propuestas eje', 0.17658258809703428), ('lean', 0.169264689128607)]\n",
      "Topic  26\n",
      "[('opcion gente', 0.5567983286432013), ('hagamos', 0.41796983244896424), ('programa municipalidad', 0.29898901889110363), ('hagamos sucedan', 0.2513115549531279), ('sucedan', 0.23156186402596443), ('suscribir', 0.15986022733359312), ('siguiente acciones', 0.14964074683158593), ('gente gestion', 0.14022899011237397), ('desarrollara siguiente', 0.13299580416424853), ('capa asfaltica', 0.11302986104055793)]\n",
      "Topic  27\n",
      "[('pa', 0.4383568676158026), ('on', 0.3429852379872556), ('zo', 0.3398700897541093), ('ej', 0.3374800677317082), ('cia', 0.29732313990773057), ('palmareno', 0.24573801163324188), ('caficultura', 0.24114828753239856), ('desamparadeno', 0.22147789055897626), ('proyectos tarifas', 0.21596345647878465), ('objetivos proyectos', 0.1953030953642574)]\n",
      "Topic  28\n",
      "[('dimension', 0.48947511733928784), ('garantizara', 0.44110073485152534), ('pam', 0.3666692628334964), ('opcion gente', 0.35404369146417536), ('traves campanas', 0.32603904363259706), ('descarbonizacion', 0.3193062505643018), ('mensual', 0.3131817103544682), ('margenes', 0.30989867920169484), ('apoyo emprendimientos', 0.28630250374863003), ('marzo', 0.2781324925377657)]\n",
      "Topic  29\n",
      "[('instruir', 0.5724192106619297), ('munici', 0.28955708670876684), ('publi', 0.2778644952047239), ('aten', 0.26820613123382747), ('cos', 0.22859467036719197), ('vas', 0.21677303234315634), ('progreso humano', 0.20726867157750023), ('empre', 0.18830828273960973), ('palidad', 0.1882148118227723), ('espa', 0.18526296878207613)]\n"
     ]
    }
   ],
   "source": [
    "# Initialize a TF-IDF Vectorizer\n",
    "# The ngram_range parameter is set to (1, 2) to include both unigrams and bigrams\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, ngram_range=(1, 2))\n",
    "\n",
    "# Transform the texts into a document-term matrix\n",
    "dtm = tfidf_vectorizer.fit_transform(plans['processed_text'])\n",
    "\n",
    "# Initialize and fit LDA\n",
    "lda = LatentDirichletAllocation(n_components=30, random_state=0)\n",
    "lda.fit(dtm)\n",
    "\n",
    "# Function to print the topics and their top words\n",
    "def print_topics(model, vectorizer, top_n=10):\n",
    "    for idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic \", idx)\n",
    "        print([(vectorizer.get_feature_names_out()[i], topic[i])\n",
    "               for i in topic.argsort()[:-top_n - 1:-1]])\n",
    "\n",
    "print_topics(lda, tfidf_vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alles over again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.matutils import Sparse2Corpus\n",
    "from gensim.models.ldamodel import LdaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Convert Sparse Matrix to Gensim's Corpus\n",
    "corpus = Sparse2Corpus(dtm, documents_columns=False)  # dtm is the document-term matrix from Scikit-Learn TF-IDF in the previous chunk\n",
    "\n",
    "# Step 2: Create Dictionary object that maps IDs to words\n",
    "id2word = corpora.Dictionary()\n",
    "id2word.token2id = tfidf_vectorizer.vocabulary_\n",
    "id2word.id2token = {v: k for k, v in tfidf_vectorizer.vocabulary_.items()}\n",
    "\n",
    "# Since the vocabulary_ of TfidfVectorizer does not preserve order, we need to sort it\n",
    "sorted_vocab = sorted(tfidf_vectorizer.vocabulary_.items(), key=lambda x: x[1])\n",
    "id2word.token2id = {key: val for key, val in sorted_vocab}\n",
    "id2word.id2token = {val: key for key, val in sorted_vocab}\n",
    "\n",
    "# Note: This assumes you can tokenize your texts in the same way they were tokenized for the original LDA\n",
    "lda_gensim = LdaModel(\n",
    "    corpus=corpus,\n",
    "    id2word=id2word,\n",
    "    num_topics=5,\n",
    "    random_state=0,\n",
    "    passes=10,\n",
    "    alpha='auto',\n",
    "    eta='auto'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "0.000*\"elementos especificos\" + 0.000*\"llevandolo\" + 0.000*\"salvemos\" + 0.000*\"elementos\" + 0.000*\"especificos\" + 0.000*\"estrategias\" + 0.000*\"promover\" + 0.000*\"colaboracion\" + 0.000*\"comunidad\" + 0.000*\"locales\"\n",
      "\n",
      "\n",
      "Topic 1:\n",
      "0.000*\"relanzar\" + 0.000*\"crear\" + 0.000*\"promover\" + 0.000*\"personas\" + 0.000*\"coordinacion\" + 0.000*\"instituciones\" + 0.000*\"implementar\" + 0.000*\"programas\" + 0.000*\"cada\" + 0.000*\"municipalidad\"\n",
      "\n",
      "\n",
      "Topic 2:\n",
      "0.000*\"gobierno\" + 0.000*\"gobierno municipalidad\" + 0.000*\"promover\" + 0.000*\"comunidad\" + 0.000*\"programa\" + 0.000*\"programas\" + 0.000*\"programa gobierno\" + 0.000*\"participacion\" + 0.000*\"educacion\" + 0.000*\"eje\"\n",
      "\n",
      "\n",
      "Topic 3:\n",
      "0.000*\"pa\" + 0.000*\"zo\" + 0.000*\"ej\" + 0.000*\"on\" + 0.000*\"cia\" + 0.000*\"dependen\" + 0.000*\"gestion\" + 0.000*\"municipalidad\" + 0.000*\"gobierno\" + 0.000*\"creacion\"\n",
      "\n",
      "\n",
      "Topic 4:\n",
      "0.000*\"quintanilla\" + 0.000*\"claudia\" + 0.000*\"salazar\" + 0.000*\"comunidad\" + 0.000*\"promover\" + 0.000*\"espacios\" + 0.000*\"participacion\" + 0.000*\"personas\" + 0.000*\"fomentar\" + 0.000*\"mejorar\"\n",
      "\n",
      "\n",
      "Topic 5:\n",
      "0.000*\"programa\" + 0.000*\"gobierno\" + 0.000*\"proyecto\" + 0.000*\"personas\" + 0.000*\"poblacion\" + 0.000*\"espacios\" + 0.000*\"gestion\" + 0.000*\"salud\" + 0.000*\"servicios\" + 0.000*\"programas\"\n",
      "\n",
      "\n",
      "Topic 6:\n",
      "0.000*\"gobierno\" + 0.000*\"personas\" + 0.000*\"eje\" + 0.000*\"gestion\" + 0.000*\"central\" + 0.000*\"asi\" + 0.000*\"comunidad\" + 0.000*\"servicios\" + 0.000*\"proyectos\" + 0.000*\"participacion\"\n",
      "\n",
      "\n",
      "Topic 7:\n",
      "0.000*\"comunidad\" + 0.000*\"gobierno\" + 0.000*\"gobierno municipal\" + 0.000*\"programas\" + 0.000*\"recursos\" + 0.000*\"mejorar\" + 0.000*\"promover\" + 0.000*\"servicios\" + 0.000*\"colaboracion\" + 0.000*\"local\"\n",
      "\n",
      "\n",
      "Topic 8:\n",
      "0.000*\"personas\" + 0.000*\"construccion\" + 0.000*\"zona maritima\" + 0.000*\"proyectos\" + 0.000*\"habilitacion\" + 0.000*\"maritima\" + 0.000*\"municipalidad\" + 0.000*\"comunidades\" + 0.000*\"salud\" + 0.000*\"municipales\"\n",
      "\n",
      "\n",
      "Topic 9:\n",
      "0.000*\"corto plazo\" + 0.000*\"corto\" + 0.000*\"plazo\" + 0.000*\"mediano\" + 0.000*\"mediano plazo\" + 0.000*\"ejecucion\" + 0.000*\"gobierno\" + 0.000*\"eje\" + 0.000*\"municipales programas\" + 0.000*\"programas gobierno\"\n",
      "\n",
      "\n",
      "Topic 10:\n",
      "0.000*\"servicios\" + 0.000*\"servicios basicos\" + 0.000*\"gestion\" + 0.000*\"basicos\" + 0.000*\"acceso servicios\" + 0.000*\"municipalidad\" + 0.000*\"identifican\" + 0.000*\"saneamiento electricidad\" + 0.000*\"asociados cambio\" + 0.000*\"degradacion\"\n",
      "\n",
      "\n",
      "Topic 11:\n",
      "0.000*\"gobierno\" + 0.000*\"promover\" + 0.000*\"adn\" + 0.000*\"espacios\" + 0.000*\"ciudad\" + 0.000*\"personas\" + 0.000*\"local\" + 0.000*\"gestion\" + 0.000*\"comunidad\" + 0.000*\"salud\"\n",
      "\n",
      "\n",
      "Topic 12:\n",
      "0.000*\"programa degobierno\" + 0.000*\"elementos estrategicos\" + 0.000*\"estrategicos programa\" + 0.000*\"degobierno\" + 0.000*\"programa\" + 0.000*\"gestion\" + 0.000*\"programas\" + 0.000*\"personas\" + 0.000*\"social\" + 0.000*\"proyectos\"\n",
      "\n",
      "\n",
      "Topic 13:\n",
      "0.000*\"aliados estrategicos\" + 0.000*\"aliados\" + 0.000*\"beneficiarios\" + 0.000*\"recursos propios\" + 0.000*\"vinculacion\" + 0.000*\"involucradas\" + 0.000*\"propios\" + 0.000*\"financiamiento\" + 0.000*\"promover\" + 0.000*\"instituciones\"\n",
      "\n",
      "\n",
      "Topic 14:\n",
      "0.000*\"sabanilla\" + 0.000*\"convenio\" + 0.000*\"gestion\" + 0.000*\"vice\" + 0.000*\"ifam\" + 0.000*\"servicios\" + 0.000*\"programa\" + 0.000*\"sostenible\" + 0.000*\"contacto\" + 0.000*\"depto\"\n",
      "\n",
      "\n",
      "Topic 15:\n",
      "0.000*\"programa gobierno\" + 0.000*\"gobierno\" + 0.000*\"municipalidad\" + 0.000*\"gestion\" + 0.000*\"programa\" + 0.000*\"personas\" + 0.000*\"medio\" + 0.000*\"ley\" + 0.000*\"instituciones\" + 0.000*\"salud\"\n",
      "\n",
      "\n",
      "Topic 16:\n",
      "0.000*\"ejercera\" + 0.000*\"municipalidad\" + 0.000*\"infraestructura\" + 0.000*\"ministerio\" + 0.000*\"gobierno\" + 0.000*\"turismo\" + 0.000*\"mediante\" + 0.000*\"nacional\" + 0.000*\"educacion\" + 0.000*\"caribe\"\n",
      "\n",
      "\n",
      "Topic 17:\n",
      "0.000*\"personas\" + 0.000*\"programas\" + 0.000*\"comunidad\" + 0.000*\"servicios\" + 0.000*\"mediante\" + 0.000*\"gestion\" + 0.000*\"proyectos\" + 0.000*\"social\" + 0.000*\"locales\" + 0.000*\"diferentes\"\n",
      "\n",
      "\n",
      "Topic 18:\n",
      "0.001*\"plp\" + 0.000*\"pregunta\" + 0.000*\"podemos hacerlo\" + 0.000*\"gobierno debe\" + 0.000*\"mil personas\" + 0.000*\"impactar\" + 0.000*\"mil\" + 0.000*\"cantonal\" + 0.000*\"hacerlo\" + 0.000*\"personas\"\n",
      "\n",
      "\n",
      "Topic 19:\n",
      "0.000*\"gobierno municipal\" + 0.000*\"municipalidad\" + 0.000*\"gobierno\" + 0.000*\"gestion\" + 0.000*\"programa gobierno\" + 0.000*\"plp\" + 0.000*\"sector\" + 0.000*\"personas\" + 0.000*\"social\" + 0.000*\"servicios\"\n",
      "\n",
      "\n",
      "Topic 20:\n",
      "0.000*\"candidato\" + 0.000*\"gobierno\" + 0.000*\"gobierno version\" + 0.000*\"medio\" + 0.000*\"plan gobierno\" + 0.000*\"programa degobierno\" + 0.000*\"version\" + 0.000*\"buscara\" + 0.000*\"elementos estrategicos\" + 0.000*\"gestion\"\n",
      "\n",
      "\n",
      "Topic 21:\n",
      "0.005*\"gestion\" + 0.004*\"gobierno\" + 0.004*\"municipalidad\" + 0.004*\"personas\" + 0.004*\"servicios\" + 0.004*\"promover\" + 0.003*\"programas\" + 0.003*\"proyectos\" + 0.003*\"local\" + 0.003*\"recursos\"\n",
      "\n",
      "\n",
      "Topic 22:\n",
      "0.000*\"gobierno\" + 0.000*\"promover\" + 0.000*\"personas\" + 0.000*\"gestion\" + 0.000*\"cantonal\" + 0.000*\"local\" + 0.000*\"creacion\" + 0.000*\"apoyo\" + 0.000*\"asi\" + 0.000*\"espacios\"\n",
      "\n",
      "\n",
      "Topic 23:\n",
      "0.000*\"municipales programas\" + 0.000*\"programas gobierno\" + 0.000*\"instruir\" + 0.000*\"ro\" + 0.000*\"ra\" + 0.000*\"gobierno municipalidad\" + 0.000*\"re\" + 0.000*\"limpia\" + 0.000*\"municipalidad\" + 0.000*\"gestion\"\n",
      "\n",
      "\n",
      "Topic 24:\n",
      "0.000*\"sostenible participativo\" + 0.000*\"pilares plan\" + 0.000*\"pilares\" + 0.000*\"plan desarrollo\" + 0.000*\"participativo\" + 0.000*\"eje\" + 0.000*\"gobierno\" + 0.000*\"diferentes\" + 0.000*\"gestion\" + 0.000*\"personas\"\n",
      "\n",
      "\n",
      "Topic 25:\n",
      "0.000*\"fi\" + 0.000*\"pueblo\" + 0.000*\"personas\" + 0.000*\"asi\" + 0.000*\"trabajo\" + 0.000*\"ramonense\" + 0.000*\"municipalidad\" + 0.000*\"ciudad\" + 0.000*\"gobierno\" + 0.000*\"servicios\"\n",
      "\n",
      "\n",
      "Topic 26:\n",
      "0.000*\"gestion\" + 0.000*\"gobierno\" + 0.000*\"servicios\" + 0.000*\"ciudad\" + 0.000*\"espacios\" + 0.000*\"construccion\" + 0.000*\"trabajo\" + 0.000*\"mejorar\" + 0.000*\"distritos\" + 0.000*\"municipalidad\"\n",
      "\n",
      "\n",
      "Topic 27:\n",
      "0.000*\"personas\" + 0.000*\"proyecto financiara\" + 0.000*\"ingresos corrientes\" + 0.000*\"servicios\" + 0.000*\"local\" + 0.000*\"recursos ordinarios\" + 0.000*\"recursos\" + 0.000*\"gobierno\" + 0.000*\"financiara\" + 0.000*\"gestion\"\n",
      "\n",
      "\n",
      "Topic 28:\n",
      "0.000*\"personas\" + 0.000*\"municipalidad\" + 0.000*\"promover\" + 0.000*\"gestion\" + 0.000*\"cantonal\" + 0.000*\"espacios\" + 0.000*\"poblacion\" + 0.000*\"crear\" + 0.000*\"servicios\" + 0.000*\"recursos\"\n",
      "\n",
      "\n",
      "Topic 29:\n",
      "0.000*\"gestion\" + 0.000*\"servicios\" + 0.000*\"mediante\" + 0.000*\"promover\" + 0.000*\"participacion\" + 0.000*\"programas\" + 0.000*\"gobierno\" + 0.000*\"calidad\" + 0.000*\"acciones\" + 0.000*\"apoyo\"\n",
      "\n",
      "\n",
      "Topic 30:\n",
      "0.000*\"proyecto\" + 0.000*\"municipalidad\" + 0.000*\"promover\" + 0.000*\"gestion\" + 0.000*\"personas\" + 0.000*\"gobierno\" + 0.000*\"comunidad\" + 0.000*\"infraestructura\" + 0.000*\"programas\" + 0.000*\"local\"\n",
      "\n",
      "\n",
      "Topic 31:\n",
      "0.000*\"municipalidad\" + 0.000*\"comunidad\" + 0.000*\"local\" + 0.000*\"gobierno\" + 0.000*\"social\" + 0.000*\"proyectos\" + 0.000*\"programas\" + 0.000*\"economico\" + 0.000*\"plan gobierno\" + 0.000*\"trabajo\"\n",
      "\n",
      "\n",
      "Topic 32:\n",
      "0.000*\"proyecto\" + 0.000*\"realizar\" + 0.000*\"promover\" + 0.000*\"gestion\" + 0.000*\"fomentar\" + 0.000*\"proyectos\" + 0.000*\"construccion\" + 0.000*\"municipalidad\" + 0.000*\"mejorar\" + 0.000*\"creacion\"\n",
      "\n",
      "\n",
      "Topic 33:\n",
      "0.000*\"esperanza\" + 0.000*\"pueblos\" + 0.000*\"libertad\" + 0.000*\"propuesta\" + 0.000*\"fin\" + 0.000*\"propuesta establecer\" + 0.000*\"gobierno\" + 0.000*\"plan gobierno\" + 0.000*\"local\" + 0.000*\"gobierno local\"\n",
      "\n",
      "\n",
      "Topic 34:\n",
      "0.000*\"progresa\" + 0.000*\"gobierno\" + 0.000*\"servicios\" + 0.000*\"local\" + 0.000*\"proyecto comunidad\" + 0.000*\"proyectos\" + 0.000*\"cantonal\" + 0.000*\"gestion\" + 0.000*\"infraestructura\" + 0.000*\"municipalidad\"\n",
      "\n",
      "\n",
      "Topic 35:\n",
      "0.000*\"tribunal interno\" + 0.000*\"tribunal\" + 0.000*\"cada\" + 0.000*\"distrito\" + 0.000*\"programa\" + 0.000*\"distritos\" + 0.000*\"central\" + 0.000*\"interno\" + 0.000*\"orotinense\" + 0.000*\"gobierno\"\n",
      "\n",
      "\n",
      "Topic 36:\n",
      "0.000*\"municipalidad\" + 0.000*\"personas\" + 0.000*\"poblacion\" + 0.000*\"proyectos\" + 0.000*\"gobierno municipalidad\" + 0.000*\"progreso humano\" + 0.000*\"atencion\" + 0.000*\"gobierno\" + 0.000*\"gestion\" + 0.000*\"servicios\"\n",
      "\n",
      "\n",
      "Topic 37:\n",
      "0.000*\"municipalidad\" + 0.000*\"gestion\" + 0.000*\"poblacion\" + 0.000*\"asi\" + 0.000*\"gobierno\" + 0.000*\"manera\" + 0.000*\"local\" + 0.000*\"programa\" + 0.000*\"espacios\" + 0.000*\"proyectos\"\n",
      "\n",
      "\n",
      "Topic 38:\n",
      "0.000*\"pnp\" + 0.000*\"estrategica\" + 0.000*\"realizar menos\" + 0.000*\"objetivos desarrollo\" + 0.000*\"menos\" + 0.000*\"personas\" + 0.000*\"gobierno\" + 0.000*\"objetivos\" + 0.000*\"plan gobierno\" + 0.000*\"poblacion\"\n",
      "\n",
      "\n",
      "Topic 39:\n",
      "0.000*\"gobierno\" + 0.000*\"gestion\" + 0.000*\"promover\" + 0.000*\"programas\" + 0.000*\"calidad\" + 0.000*\"local\" + 0.000*\"servicios\" + 0.000*\"vida\" + 0.000*\"municipal trabajando\" + 0.000*\"medio\"\n",
      "\n",
      "\n",
      "Topic 40:\n",
      "0.000*\"gobierno\" + 0.000*\"adn\" + 0.000*\"municipalidad\" + 0.000*\"gestion\" + 0.000*\"cantonal\" + 0.000*\"personas\" + 0.000*\"debe\" + 0.000*\"social\" + 0.000*\"programa\" + 0.000*\"servicios\"\n",
      "\n",
      "\n",
      "Topic 41:\n",
      "0.000*\"as\" + 0.000*\"debe\" + 0.000*\"local\" + 0.000*\"social\" + 0.000*\"gobierno\" + 0.000*\"comunidad\" + 0.000*\"programas\" + 0.000*\"ser\" + 0.000*\"sector\" + 0.000*\"espacios\"\n",
      "\n",
      "\n",
      "Topic 42:\n",
      "0.000*\"responsable municipal\" + 0.000*\"municipal afectado\" + 0.000*\"afectado\" + 0.000*\"indice gestion\" + 0.000*\"gestion\" + 0.000*\"menos\" + 0.000*\"ano\" + 0.000*\"promover\" + 0.000*\"aspecto\" + 0.000*\"especifico\"\n",
      "\n",
      "\n",
      "Topic 43:\n",
      "0.000*\"optar\" + 0.000*\"operativo\" + 0.000*\"programacion\" + 0.000*\"propuesta\" + 0.000*\"pensando grande\" + 0.000*\"gestion\" + 0.000*\"pln\" + 0.000*\"pensando\" + 0.000*\"grande\" + 0.000*\"programas\"\n",
      "\n",
      "\n",
      "Topic 44:\n",
      "0.000*\"gobierno estrategicos\" + 0.000*\"articulacion interinstitucional\" + 0.000*\"promover\" + 0.000*\"gobierno\" + 0.000*\"personas\" + 0.000*\"plan gobierno\" + 0.000*\"acciones\" + 0.000*\"gestion\" + 0.000*\"poblacion\" + 0.000*\"servicios\"\n",
      "\n",
      "\n",
      "Topic 45:\n",
      "0.000*\"programacion\" + 0.000*\"tecnica\" + 0.000*\"comision\" + 0.000*\"presupuesto\" + 0.000*\"evaluacion\" + 0.000*\"estrategia evaluacion\" + 0.000*\"gestion\" + 0.000*\"eje\" + 0.000*\"personas\" + 0.000*\"programacion evaluacion\"\n",
      "\n",
      "\n",
      "Topic 46:\n",
      "0.000*\"personas\" + 0.000*\"gobierno\" + 0.000*\"gestion\" + 0.000*\"municipalidad\" + 0.000*\"asi\" + 0.000*\"local\" + 0.000*\"servicios\" + 0.000*\"recursos\" + 0.000*\"plan gobierno\" + 0.000*\"promover\"\n",
      "\n",
      "\n",
      "Topic 47:\n",
      "0.000*\"descripcion\" + 0.000*\"acciones\" + 0.000*\"ano\" + 0.000*\"construccion\" + 0.000*\"gobierno\" + 0.000*\"personas\" + 0.000*\"servicios\" + 0.000*\"municipalidad\" + 0.000*\"gestion\" + 0.000*\"infraestructura\"\n",
      "\n",
      "\n",
      "Topic 48:\n",
      "0.001*\"opcion gente\" + 0.000*\"opcion\" + 0.000*\"gente\" + 0.000*\"lograrlo\" + 0.000*\"propuesta\" + 0.000*\"elaprovechamiento recursos\" + 0.000*\"personas\" + 0.000*\"gestion\" + 0.000*\"puente entrada\" + 0.000*\"ict canatur\"\n",
      "\n",
      "\n",
      "Topic 49:\n",
      "0.001*\"tribunal interno\" + 0.001*\"tribunal\" + 0.000*\"interno\" + 0.000*\"programas\" + 0.000*\"proyectos\" + 0.000*\"apoyo\" + 0.000*\"espacios\" + 0.000*\"comunidad\" + 0.000*\"personas\" + 0.000*\"crear\"\n",
      "\n",
      "\n",
      "Topic 50:\n",
      "0.000*\"personas\" + 0.000*\"gobierno\" + 0.000*\"plan gobierno\" + 0.000*\"promover\" + 0.000*\"central\" + 0.000*\"gestion\" + 0.000*\"salud\" + 0.000*\"alianzas\" + 0.000*\"comunidad\" + 0.000*\"ods\"\n",
      "\n",
      "\n",
      "Topic 51:\n",
      "0.000*\"tribunal interno\" + 0.000*\"pm\" + 0.000*\"tribunal\" + 0.000*\"ambiente\" + 0.000*\"merecedor\" + 0.000*\"grupo tan\" + 0.000*\"clara vision\" + 0.000*\"ludico\" + 0.000*\"oportunidades medio\" + 0.000*\"constitucional circunscripcion\"\n",
      "\n",
      "\n",
      "Topic 52:\n",
      "0.000*\"comunidad\" + 0.000*\"ciudadanos\" + 0.000*\"municipalidad\" + 0.000*\"proyectos\" + 0.000*\"programa\" + 0.000*\"espacios\" + 0.000*\"ciudad\" + 0.000*\"puertas abiertas\" + 0.000*\"gestion\" + 0.000*\"gobierno\"\n",
      "\n",
      "\n",
      "Topic 53:\n",
      "0.000*\"gestion\" + 0.000*\"municipalidad\" + 0.000*\"eje\" + 0.000*\"mediante\" + 0.000*\"promover\" + 0.000*\"realizar\" + 0.000*\"programas\" + 0.000*\"recursos\" + 0.000*\"mejorar\" + 0.000*\"turismo\"\n",
      "\n",
      "\n",
      "Topic 54:\n",
      "0.000*\"justa\" + 0.000*\"personas\" + 0.000*\"municipalidad\" + 0.000*\"fin\" + 0.000*\"gobierno\" + 0.000*\"atencion\" + 0.000*\"asi\" + 0.000*\"patrimonio\" + 0.000*\"municipales\" + 0.000*\"comunidades\"\n",
      "\n",
      "\n",
      "Topic 55:\n",
      "0.000*\"programa\" + 0.000*\"gestion\" + 0.000*\"social\" + 0.000*\"programade\" + 0.000*\"personas\" + 0.000*\"sostenible\" + 0.000*\"municipalidad\" + 0.000*\"local\" + 0.000*\"vida\" + 0.000*\"equipo plp\"\n",
      "\n",
      "\n",
      "Topic 56:\n",
      "0.000*\"turismo\" + 0.000*\"social\" + 0.000*\"locales\" + 0.000*\"local\" + 0.000*\"promover\" + 0.000*\"ley\" + 0.000*\"plp\" + 0.000*\"gestion\" + 0.000*\"gobierno\" + 0.000*\"vial\"\n",
      "\n",
      "\n",
      "Topic 57:\n",
      "0.000*\"manuel antonio\" + 0.000*\"manuel\" + 0.000*\"antonio\" + 0.000*\"gobierno\" + 0.000*\"social\" + 0.000*\"gestion\" + 0.000*\"programas\" + 0.000*\"personas\" + 0.000*\"sostenible\" + 0.000*\"proyecto\"\n",
      "\n",
      "\n",
      "Topic 58:\n",
      "0.000*\"municipal tiempo\" + 0.000*\"propuesta gobierno\" + 0.000*\"socio economico\" + 0.000*\"socio\" + 0.000*\"economico sostenible\" + 0.000*\"cambiar\" + 0.000*\"comunidad\" + 0.000*\"desarrollo humano\" + 0.000*\"programas\" + 0.000*\"gobierno\"\n",
      "\n",
      "\n",
      "Topic 59:\n",
      "0.000*\"municipalidad altura\" + 0.000*\"altura\" + 0.000*\"municipalidad\" + 0.000*\"debe\" + 0.000*\"proyectos\" + 0.000*\"mediante\" + 0.000*\"gobierno\" + 0.000*\"personas\" + 0.000*\"penal\" + 0.000*\"promover\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Printing topics\n",
    "\n",
    "def display_topics(model, num_words=10):\n",
    "    for topic_id, topic in model.print_topics(num_topics=-1, num_words=num_words):\n",
    "        print(f\"Topic {topic_id}:\")\n",
    "        print(topic)\n",
    "        print(\"\\n\")\n",
    "\n",
    "display_topics(lda_gensim, num_words=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Coherence Score (C_V):  nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\carme\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gensim\\topic_coherence\\direct_confirmation_measure.py:204: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  m_lr_i = np.log(numerator / denominator)\n",
      "c:\\Users\\carme\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gensim\\topic_coherence\\indirect_confirmation_measure.py:323: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  return cv1.T.dot(cv2)[0, 0] / (_magnitude(cv1) * _magnitude(cv2))\n"
     ]
    }
   ],
   "source": [
    "# Tokenize your texts. This should match whatever tokenizer you used to prepare your DTM for LDA\n",
    "tokenized_texts = [doc.split() for doc in plans['processed_text']]  # Assuming the text is space-separated tokens\n",
    "\n",
    "# Compute Coherence Score using C_V measure\n",
    "coherence_model_lda = CoherenceModel(model=lda_gensim, texts=tokenized_texts, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score (C_V): ', coherence_lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Donx ChatGPT acerca de documentos que son muy parecidos:\n",
    "\n",
    "Yes, when documents within your corpus are very similar and cover overlapping subjects, traditional LDA can struggle to differentiate distinct topics effectively. This is because LDA relies on variations in word frequencies across different documents to discern topics. If most documents share a similar vocabulary and thematic content, the resulting topics can be less distinct and more mixed, which might not be very helpful for your analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERTopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bertopicNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading bertopic-0.16.0-py2.py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: numpy>=1.20.0 in c:\\users\\carme\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from bertopic) (1.26.2)\n",
      "Collecting hdbscan>=0.8.29 (from bertopic)\n",
      "  Downloading hdbscan-0.8.33.tar.gz (5.2 MB)\n",
      "     ---------------------------------------- 0.0/5.2 MB ? eta -:--:--\n",
      "     - -------------------------------------- 0.2/5.2 MB 4.8 MB/s eta 0:00:02\n",
      "     ---- ----------------------------------- 0.6/5.2 MB 7.0 MB/s eta 0:00:01\n",
      "     -------- ------------------------------- 1.1/5.2 MB 8.5 MB/s eta 0:00:01\n",
      "     -------------- ------------------------- 1.9/5.2 MB 10.8 MB/s eta 0:00:01\n",
      "     ----------------- ---------------------- 2.3/5.2 MB 11.2 MB/s eta 0:00:01\n",
      "     --------------------- ------------------ 2.8/5.2 MB 11.1 MB/s eta 0:00:01\n",
      "     ------------------------- -------------- 3.3/5.2 MB 11.7 MB/s eta 0:00:01\n",
      "     ------------------------------- -------- 4.0/5.2 MB 11.2 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 4.7/5.2 MB 11.6 MB/s eta 0:00:01\n",
      "     ---------------------------------------  5.2/5.2 MB 11.8 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 5.2/5.2 MB 11.1 MB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: umap-learn>=0.5.0 in c:\\users\\carme\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from bertopic) (0.5.5)\n",
      "Requirement already satisfied: pandas>=1.1.5 in c:\\users\\carme\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from bertopic) (2.1.3)\n",
      "Requirement already satisfied: scikit-learn>=0.22.2.post1 in c:\\users\\carme\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from bertopic) (1.3.2)\n",
      "Requirement already satisfied: tqdm>=4.41.1 in c:\\users\\carme\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from bertopic) (4.66.1)\n",
      "Collecting sentence-transformers>=0.4.1 (from bertopic)\n",
      "  Downloading sentence_transformers-2.6.1-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting plotly>=4.7.0 (from bertopic)\n",
      "  Downloading plotly-5.20.0-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting cython<3,>=0.27 (from hdbscan>=0.8.29->bertopic)\n",
      "  Using cached Cython-0.29.37-py2.py3-none-any.whl.metadata (3.1 kB)\n",
      "Requirement already satisfied: scipy>=1.0 in c:\\users\\carme\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from hdbscan>=0.8.29->bertopic) (1.11.4)\n",
      "Requirement already satisfied: joblib>=1.0 in c:\\users\\carme\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from hdbscan>=0.8.29->bertopic) (1.3.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\carme\\appdata\\roaming\\python\\python310\\site-packages (from pandas>=1.1.5->bertopic) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\carme\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas>=1.1.5->bertopic) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\carme\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas>=1.1.5->bertopic) (2023.3)\n",
      "Collecting tenacity>=6.2.0 (from plotly>=4.7.0->bertopic)\n",
      "  Downloading tenacity-8.2.3-py3-none-any.whl.metadata (1.0 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\carme\\appdata\\roaming\\python\\python310\\site-packages (from plotly>=4.7.0->bertopic) (23.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\carme\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn>=0.22.2.post1->bertopic) (3.2.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.32.0 in c:\\users\\carme\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sentence-transformers>=0.4.1->bertopic) (4.35.2)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\carme\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sentence-transformers>=0.4.1->bertopic) (2.1.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.15.1 in c:\\users\\carme\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sentence-transformers>=0.4.1->bertopic) (0.19.4)\n",
      "Requirement already satisfied: Pillow in c:\\users\\carme\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sentence-transformers>=0.4.1->bertopic) (10.1.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\carme\\appdata\\roaming\\python\\python310\\site-packages (from tqdm>=4.41.1->bertopic) (0.4.6)\n",
      "Requirement already satisfied: numba>=0.51.2 in c:\\users\\carme\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from umap-learn>=0.5.0->bertopic) (0.58.1)\n",
      "Requirement already satisfied: pynndescent>=0.5 in c:\\users\\carme\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from umap-learn>=0.5.0->bertopic) (0.5.11)\n",
      "Requirement already satisfied: filelock in c:\\users\\carme\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub>=0.15.1->sentence-transformers>=0.4.1->bertopic) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\carme\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub>=0.15.1->sentence-transformers>=0.4.1->bertopic) (2023.10.0)\n",
      "Requirement already satisfied: requests in c:\\users\\carme\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub>=0.15.1->sentence-transformers>=0.4.1->bertopic) (2.31.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\carme\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub>=0.15.1->sentence-transformers>=0.4.1->bertopic) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\carme\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub>=0.15.1->sentence-transformers>=0.4.1->bertopic) (4.8.0)\n",
      "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in c:\\users\\carme\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from numba>=0.51.2->umap-learn>=0.5.0->bertopic) (0.41.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\carme\\appdata\\roaming\\python\\python310\\site-packages (from python-dateutil>=2.8.2->pandas>=1.1.5->bertopic) (1.16.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\carme\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\carme\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\carme\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (3.1.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\carme\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers>=0.4.1->bertopic) (2023.10.3)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\users\\carme\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers>=0.4.1->bertopic) (0.15.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\carme\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers>=0.4.1->bertopic) (0.4.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\carme\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\carme\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers>=0.4.1->bertopic) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\carme\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers>=0.4.1->bertopic) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\carme\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers>=0.4.1->bertopic) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\carme\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers>=0.4.1->bertopic) (2023.11.17)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\carme\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sympy->torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (1.3.0)\n",
      "Downloading bertopic-0.16.0-py2.py3-none-any.whl (154 kB)\n",
      "   ---------------------------------------- 0.0/154.1 kB ? eta -:--:--\n",
      "   ---------------------------------------- 154.1/154.1 kB 9.0 MB/s eta 0:00:00\n",
      "Downloading plotly-5.20.0-py3-none-any.whl (15.7 MB)\n",
      "   ---------------------------------------- 0.0/15.7 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.5/15.7 MB 16.3 MB/s eta 0:00:01\n",
      "   - -------------------------------------- 0.8/15.7 MB 8.2 MB/s eta 0:00:02\n",
      "   --- ------------------------------------ 1.4/15.7 MB 8.6 MB/s eta 0:00:02\n",
      "   ---- ----------------------------------- 1.6/15.7 MB 7.7 MB/s eta 0:00:02\n",
      "   ---- ----------------------------------- 1.8/15.7 MB 8.0 MB/s eta 0:00:02\n",
      "   ---- ----------------------------------- 1.8/15.7 MB 8.0 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 2.8/15.7 MB 8.5 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 3.0/15.7 MB 7.9 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 3.5/15.7 MB 8.2 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 3.6/15.7 MB 8.2 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 4.1/15.7 MB 7.8 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 4.8/15.7 MB 8.4 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 4.9/15.7 MB 8.4 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 5.4/15.7 MB 8.2 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 5.7/15.7 MB 7.7 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 6.0/15.7 MB 8.0 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 6.1/15.7 MB 7.8 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 6.1/15.7 MB 7.8 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 6.7/15.7 MB 7.6 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 7.1/15.7 MB 7.5 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 7.5/15.7 MB 7.6 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 7.9/15.7 MB 7.7 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 8.2/15.7 MB 7.7 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 8.5/15.7 MB 7.6 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 9.0/15.7 MB 7.6 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 9.0/15.7 MB 7.6 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 9.0/15.7 MB 7.2 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 9.1/15.7 MB 7.1 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 9.1/15.7 MB 6.7 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 9.1/15.7 MB 6.7 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 9.2/15.7 MB 6.4 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 9.5/15.7 MB 6.4 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 9.7/15.7 MB 6.3 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 9.7/15.7 MB 6.1 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 9.8/15.7 MB 6.0 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 9.9/15.7 MB 5.9 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 10.1/15.7 MB 5.9 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 10.4/15.7 MB 5.8 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 10.4/15.7 MB 5.8 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 10.6/15.7 MB 5.6 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 10.9/15.7 MB 5.6 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 11.2/15.7 MB 5.5 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 11.5/15.7 MB 5.5 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 11.7/15.7 MB 5.4 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 12.0/15.7 MB 5.5 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 12.1/15.7 MB 5.6 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 12.6/15.7 MB 5.5 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 12.7/15.7 MB 5.4 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 13.0/15.7 MB 5.3 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 13.4/15.7 MB 5.4 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 13.7/15.7 MB 5.4 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 14.1/15.7 MB 5.5 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 14.2/15.7 MB 5.3 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 14.4/15.7 MB 5.3 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 14.4/15.7 MB 5.3 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 14.4/15.7 MB 5.3 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 14.9/15.7 MB 5.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 14.9/15.7 MB 4.8 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 15.0/15.7 MB 4.8 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 15.1/15.7 MB 4.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  15.3/15.7 MB 4.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  15.4/15.7 MB 4.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  15.7/15.7 MB 4.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 15.7/15.7 MB 4.6 MB/s eta 0:00:00\n",
      "Downloading sentence_transformers-2.6.1-py3-none-any.whl (163 kB)\n",
      "   ---------------------------------------- 0.0/163.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 163.3/163.3 kB 4.8 MB/s eta 0:00:00\n",
      "Using cached Cython-0.29.37-py2.py3-none-any.whl (989 kB)\n",
      "Downloading tenacity-8.2.3-py3-none-any.whl (24 kB)\n",
      "Building wheels for collected packages: hdbscan\n",
      "  Building wheel for hdbscan (pyproject.toml): started\n",
      "  Building wheel for hdbscan (pyproject.toml): finished with status 'error'\n",
      "Failed to build hdbscan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × Building wheel for hdbscan (pyproject.toml) did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [40 lines of output]\n",
      "      running bdist_wheel\n",
      "      running build\n",
      "      running build_py\n",
      "      creating build\n",
      "      creating build\\lib.win-amd64-cpython-310\n",
      "      creating build\\lib.win-amd64-cpython-310\\hdbscan\n",
      "      copying hdbscan\\flat.py -> build\\lib.win-amd64-cpython-310\\hdbscan\n",
      "      copying hdbscan\\hdbscan_.py -> build\\lib.win-amd64-cpython-310\\hdbscan\n",
      "      copying hdbscan\\plots.py -> build\\lib.win-amd64-cpython-310\\hdbscan\n",
      "      copying hdbscan\\prediction.py -> build\\lib.win-amd64-cpython-310\\hdbscan\n",
      "      copying hdbscan\\robust_single_linkage_.py -> build\\lib.win-amd64-cpython-310\\hdbscan\n",
      "      copying hdbscan\\validity.py -> build\\lib.win-amd64-cpython-310\\hdbscan\n",
      "      copying hdbscan\\__init__.py -> build\\lib.win-amd64-cpython-310\\hdbscan\n",
      "      creating build\\lib.win-amd64-cpython-310\\hdbscan\\tests\n",
      "      copying hdbscan\\tests\\test_flat.py -> build\\lib.win-amd64-cpython-310\\hdbscan\\tests\n",
      "      copying hdbscan\\tests\\test_hdbscan.py -> build\\lib.win-amd64-cpython-310\\hdbscan\\tests\n",
      "      copying hdbscan\\tests\\test_prediction_utils.py -> build\\lib.win-amd64-cpython-310\\hdbscan\\tests\n",
      "      copying hdbscan\\tests\\test_rsl.py -> build\\lib.win-amd64-cpython-310\\hdbscan\\tests\n",
      "      copying hdbscan\\tests\\__init__.py -> build\\lib.win-amd64-cpython-310\\hdbscan\\tests\n",
      "      running build_ext\n",
      "      cythoning hdbscan/_hdbscan_tree.pyx to hdbscan\\_hdbscan_tree.c\n",
      "      C:\\Users\\carme\\AppData\\Local\\Temp\\pip-build-env-_peqbrve\\overlay\\Lib\\site-packages\\Cython\\Compiler\\Main.py:369: FutureWarning: Cython directive 'language_level' not set, using 2 for now (Py2). This will change in a later release! File: C:\\Users\\carme\\AppData\\Local\\Temp\\pip-install-0hyzpk_u\\hdbscan_85006f2713f245ffbbadab41357605ac\\hdbscan\\_hdbscan_tree.pyx\n",
      "        tree = Parsing.p_module(s, pxd, full_module_name)\n",
      "      cythoning hdbscan/_hdbscan_linkage.pyx to hdbscan\\_hdbscan_linkage.c\n",
      "      C:\\Users\\carme\\AppData\\Local\\Temp\\pip-build-env-_peqbrve\\overlay\\Lib\\site-packages\\Cython\\Compiler\\Main.py:369: FutureWarning: Cython directive 'language_level' not set, using 2 for now (Py2). This will change in a later release! File: C:\\Users\\carme\\AppData\\Local\\Temp\\pip-install-0hyzpk_u\\hdbscan_85006f2713f245ffbbadab41357605ac\\hdbscan\\_hdbscan_linkage.pyx\n",
      "        tree = Parsing.p_module(s, pxd, full_module_name)\n",
      "      cythoning hdbscan/_hdbscan_boruvka.pyx to hdbscan\\_hdbscan_boruvka.c\n",
      "      C:\\Users\\carme\\AppData\\Local\\Temp\\pip-build-env-_peqbrve\\overlay\\Lib\\site-packages\\Cython\\Compiler\\Main.py:369: FutureWarning: Cython directive 'language_level' not set, using 2 for now (Py2). This will change in a later release! File: C:\\Users\\carme\\AppData\\Local\\Temp\\pip-install-0hyzpk_u\\hdbscan_85006f2713f245ffbbadab41357605ac\\hdbscan\\_hdbscan_boruvka.pyx\n",
      "        tree = Parsing.p_module(s, pxd, full_module_name)\n",
      "      cythoning hdbscan/_hdbscan_reachability.pyx to hdbscan\\_hdbscan_reachability.c\n",
      "      C:\\Users\\carme\\AppData\\Local\\Temp\\pip-build-env-_peqbrve\\overlay\\Lib\\site-packages\\Cython\\Compiler\\Main.py:369: FutureWarning: Cython directive 'language_level' not set, using 2 for now (Py2). This will change in a later release! File: C:\\Users\\carme\\AppData\\Local\\Temp\\pip-install-0hyzpk_u\\hdbscan_85006f2713f245ffbbadab41357605ac\\hdbscan\\_hdbscan_reachability.pyx\n",
      "        tree = Parsing.p_module(s, pxd, full_module_name)\n",
      "      cythoning hdbscan/_prediction_utils.pyx to hdbscan\\_prediction_utils.c\n",
      "      C:\\Users\\carme\\AppData\\Local\\Temp\\pip-build-env-_peqbrve\\overlay\\Lib\\site-packages\\Cython\\Compiler\\Main.py:369: FutureWarning: Cython directive 'language_level' not set, using 2 for now (Py2). This will change in a later release! File: C:\\Users\\carme\\AppData\\Local\\Temp\\pip-install-0hyzpk_u\\hdbscan_85006f2713f245ffbbadab41357605ac\\hdbscan\\_prediction_utils.pyx\n",
      "        tree = Parsing.p_module(s, pxd, full_module_name)\n",
      "      cythoning hdbscan/dist_metrics.pyx to hdbscan\\dist_metrics.c\n",
      "      C:\\Users\\carme\\AppData\\Local\\Temp\\pip-build-env-_peqbrve\\overlay\\Lib\\site-packages\\Cython\\Compiler\\Main.py:369: FutureWarning: Cython directive 'language_level' not set, using 2 for now (Py2). This will change in a later release! File: C:\\Users\\carme\\AppData\\Local\\Temp\\pip-install-0hyzpk_u\\hdbscan_85006f2713f245ffbbadab41357605ac\\hdbscan\\dist_metrics.pxd\n",
      "        tree = Parsing.p_module(s, pxd, full_module_name)\n",
      "      building 'hdbscan._hdbscan_tree' extension\n",
      "      error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for hdbscan\n",
      "ERROR: Could not build wheels for hdbscan, which is required to install pyproject.toml-based projects\n",
      "\n",
      "[notice] A new release of pip is available: 23.3.2 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "#pip install bertopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'bertopic'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[80], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#Import model libraries #~1min\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbertopic\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BERTopic\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mumap\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m UMAP\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'bertopic'"
     ]
    }
   ],
   "source": [
    "#Import model libraries #~1min\n",
    "from bertopic import BERTopic\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "from bertopic.vectorizers import ClassTfidfTransformer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
